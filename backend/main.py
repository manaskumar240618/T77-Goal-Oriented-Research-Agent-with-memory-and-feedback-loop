from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from agent import app as agent_app
from langchain_core.messages import HumanMessage
import uvicorn
import re
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.utils import simpleSplit

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class Request(BaseModel):
    query: str
    thread_id: str
    critical_mode: bool = False 

class ReportRequest(BaseModel):
    chat_history: list

@app.get("/")
def read_root():
    return {"status": "active", "message": "Intellica Speed Backend Running"}

# --- ASYNC UPDATE ---
# We use 'async def' and 'await' to free up server resources during search
@app.post("/chat")
async def chat_endpoint(request: Request):
    try:
        user_message = HumanMessage(content=request.query)
        
        config = {
            "configurable": {
                "thread_id": request.thread_id,
                "critical_mode": request.critical_mode 
            }
        }
        
        # Use .ainvoke instead of .invoke for async speed
        result = await agent_app.ainvoke(
            {"messages": [user_message]}, 
            config=config
        )
        
        raw_content = result["messages"][-1].content
        
        suggestions = []
        suggestion_match = re.search(r"Suggestions:\s*\[(.*?)\]", raw_content, re.DOTALL)
        clean_response = raw_content
        
        if suggestion_match:
            suggestions_str = suggestion_match.group(1)
            suggestions = [s.strip() for s in suggestions_str.split(',')]
            clean_response = raw_content.replace(suggestion_match.group(0), "").strip()
        
        return {
            "response": clean_response, 
            "suggestions": suggestions
        }
        
    except Exception as e:
        print(f"Error: {e}")
        return {"response": f"Internal Error: {str(e)}", "suggestions": []}

@app.post("/generate_report")
def generate_report(request: ReportRequest):
    filename = "Intellica_Report.pdf"
    c = canvas.Canvas(filename, pagesize=letter)
    width, height = letter
    y = height - 50
    
    c.setFont("Helvetica-Bold", 16)
    c.drawString(50, y, "Intellica Research Report")
    y -= 30
    c.setFont("Helvetica", 10)
    c.drawString(50, y, "Generated by Intellica AI")
    y -= 40
    
    for msg in request.chat_history:
        if y < 50:
            c.showPage()
            y = height - 50
            
        role = "USER" if msg['role'] == 'user' else "INTELLICA AI"
        c.setFont("Helvetica-Bold", 12)
        c.drawString(50, y, f"{role}:")
        y -= 20
        
        c.setFont("Helvetica", 11)
        text = msg['content']
        lines = simpleSplit(text, "Helvetica", 11, width - 100)
        
        for line in lines:
            if y < 50:
                c.showPage()
                y = height - 50
            c.drawString(50, y, line)
            y -= 15
        y -= 20
        
    c.save()
    return FileResponse(filename, filename=filename, media_type='application/pdf')

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)